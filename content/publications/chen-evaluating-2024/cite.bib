@misc{chen_evaluating_2024,
 abstract = {As multimodal large language models (MLLMs) advance rapidly, rigorous evaluation has become essential, providing further guidance for their development. In this work, we focus on a unified and robust evaluation of \textbf\vision perception\ abilities, the foundational skill of MLLMs. We find that existing perception benchmarks, each focusing on different question types, domains, and evaluation metrics, introduce significant evaluation variance, complicating comprehensive assessments of perception abilities when relying on any single benchmark. To address this, we introduce \textbf\AbilityLens\, a unified benchmark designed to evaluate MLLMs across six key perception abilities, focusing on both accuracy and stability, with each ability encompassing diverse question types, domains, and metrics. With the assistance of AbilityLens, we: (1) identify the strengths and weaknesses of current models, highlighting stability patterns and revealing a notable performance gap between open-source and closed-source models; (2) introduce an online evaluation mode, which uncovers interesting ability conflict and early convergence phenomena during MLLM training; and (3) design a simple ability-specific model merging method that combines the best ability checkpoint from early training stages, effectively mitigating performance decline due to ability conflict. The benchmark and online leaderboard will be released soon.},
 author = {Chen, Feng and Gou, Chenhui and Liu, Jing and Yang, Yang and Li, Zhaoyang and Zhang, Jiyuan and Sun, Zhenbang and Zhuang, Bohan and Wu, Qi},
 doi = {10.48550/arXiv.2411.14725},
 month = {November},
 note = {arXiv:2411.14725 [cs]},
 publisher = {arXiv},
 title = {Evaluating and Advancing Multimodal Large Language Models in Ability Lens},
 url = {http://arxiv.org/abs/2411.14725},
 urldate = {2025-03-14},
 year = {2024}
}
