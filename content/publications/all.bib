@inproceedings{NEURIPS2023_2aab8a76,
 author = {He, Yefei and Liu, Luping and Liu, Jing and Wu, Weijia and Zhou, Hong and Zhuang, Bohan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {13237--13249},
 publisher = {Curran Associates, Inc.},
 title = {PTQD: Accurate Post-Training Quantization for Diffusion Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/2aab8a76c7e761b66eccaca0927787de-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}


@inproceedings{10.24963/ijcai.2023/764,
author = {Zhuang, Bohan and Liu, Jing and Pan, Zizheng and He, Haoyu and Weng, Yuetian and Shen, Chunhua},
title = {A survey on efficient training of transformers},
year = {2023},
isbn = {978-1-956792-03-4},
url = {https://doi.org/10.24963/ijcai.2023/764},
doi = {10.24963/ijcai.2023/764},
abstract = {Recent advances in Transformers have come with a huge requirement on computing resources, highlighting the importance of developing efficient training techniques to make Transformer training faster, at lower cost, and to higher accuracy by the efficient use of computation and memory resources. This survey provides the first systematic overview of the efficient training of Transformers, covering the recent progress in acceleration arithmetic and hardware, with a focus on the former. We analyze and compare methods that save computation and memory costs for intermediate tensors during training, together with techniques on hardware/ algorithm co-design. We finally discuss challenges and promising areas for future research.},
booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
articleno = {764},
numpages = {9},
location = {Macao, P.R.China},
series = {IJCAI '23}
}



@inproceedings{liu2024qllm,
  title = {{QLLM}: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models},
  author = {Liu, Jing and Gong, Ruihao and Wei, Xiuying and Dong, Zhiwei and Cai, Jianfei and Zhuang, Bohan},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year = {2024},
}

@inproceedings{he2024efficientdm,
  title={EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models},
  author={He, Yefei and Liu, Jing and Wu, Weijia and Zhou, Hong and Zhuang, Bohan},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@article{he2024zipcache,
  title={ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification},
  author={He, Yefei and Zhang, Luoming and Wu, Weijia and Liu, Jing and Zhou, Hong and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2405.14256},
  year={2024}
}

@misc{liu_sharpness-aware_2023,
	title = {Sharpness-aware {Quantization} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2111.12273},
	doi = {10.48550/arXiv.2111.12273},
	abstract = {Network quantization is a dominant paradigm of model compression. However, the abrupt changes in quantized weights during training often lead to severe loss fluctuations and result in a sharp loss landscape, making the gradients unstable and thus degrading the performance. Recently, Sharpness-Aware Minimization (SAM) has been proposed to smooth the loss landscape and improve the generalization performance of the models. Nevertheless, directly applying SAM to the quantized models can lead to perturbation mismatch or diminishment issues, resulting in suboptimal performance. In this paper, we propose a novel method, dubbed Sharpness-Aware Quantization (SAQ), to explore the effect of SAM in model compression, particularly quantization for the first time. Specifically, we first provide a unified view of quantization and SAM by treating them as introducing quantization noises and adversarial perturbations to the model weights, respectively. According to whether the noise and perturbation terms depend on each other, SAQ can be formulated into three cases, which are analyzed and compared comprehensively. Furthermore, by introducing an efficient training strategy, SAQ only incurs a little additional training overhead compared with the default optimizer (e.g., SGD or AdamW). Extensive experiments on both convolutional neural networks and Transformers across various datasets (i.e., ImageNet, CIFAR-10/100, Oxford Flowers-102, Oxford-IIIT Pets) show that SAQ improves the generalization performance of the quantized models, yielding the SOTA results in uniform quantization. For example, on ImageNet, SAQ outperforms AdamW by 1.2\% on the Top-1 accuracy for 4-bit ViT-B/16. Our 4-bit ResNet-50 surpasses the previous SOTA method by 0.9\% on the Top-1 accuracy.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Liu, Jing and Cai, Jianfei and Zhuang, Bohan},
	month = mar,
	year = {2023},
	note = {arXiv:2111.12273 [cs]},
}

@misc{pan_mesa_2022,
	title = {Mesa: {A} {Memory}-saving {Training} {Framework} for {Transformers}},
	shorttitle = {Mesa},
	url = {http://arxiv.org/abs/2111.11124},
	doi = {10.48550/arXiv.2111.11124},
	abstract = {There has been an explosion of interest in designing high-performance Transformers. While Transformers have delivered significant performance improvements, training such networks is extremely memory intensive owing to storing all intermediate activations that are needed for gradient computation during backpropagation, especially for long sequences. To this end, we present Mesa, a memory-saving training framework for Transformers. Specifically, Mesa uses exact activations during forward pass while storing a low-precision version of activations to reduce memory consumption during training. The low-precision activations are then dequantized during back-propagation to compute gradients. Besides, to address the heterogeneous activation distributions in the multi-head self-attention layers, we propose a head-wise activation quantization strategy, which quantizes activations based on the statistics of each head to minimize the approximation error. To further boost training efficiency, we learn quantization parameters by running estimates. More importantly, by re-investing the saved memory in employing a larger batch size or scaling up model size, we may further improve the performance under constrained computational resources. Extensive experiments on ImageNet, CIFAR-100 and ADE20K demonstrate that Mesa can achieve flexible memory-savings (up to 50\%) during training while achieving comparable or even better performance. Code is available at https://github.com/ziplab/Mesa.},
	language = {en-US},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Pan, Zizheng and Chen, Peng and He, Haoyu and Liu, Jing and Cai, Jianfei and Zhuang, Bohan},
	month = aug,
	year = {2022},
	note = {arXiv:2111.11124 [cs]},
}

@misc{pan_t-stitch_2024,
	title = {T-{Stitch}: {Accelerating} {Sampling} in {Pre}-{Trained} {Diffusion} {Models} with {Trajectory} {Stitching}},
	shorttitle = {T-{Stitch}},
	url = {http://arxiv.org/abs/2402.14167},
	doi = {10.48550/arXiv.2402.14167},
	abstract = {Sampling from diffusion probabilistic models (DPMs) is often expensive for high-quality image generation and typically requires many steps with a large model. In this paper, we introduce sampling Trajectory Stitching T-Stitch, a simple yet efficient technique to improve the sampling efficiency with little or no generation degradation. Instead of solely using a large DPM for the entire sampling trajectory, T-Stitch first leverages a smaller DPM in the initial steps as a cheap drop-in replacement of the larger DPM and switches to the larger DPM at a later stage. Our key insight is that different diffusion models learn similar encodings under the same training data distribution and smaller models are capable of generating good global structures in the early steps. Extensive experiments demonstrate that T-Stitch is training-free, generally applicable for different architectures, and complements most existing fast sampling techniques with flexible speed and quality trade-offs. On DiT-XL, for example, 40\% of the early timesteps can be safely replaced with a 10x faster DiT-S without performance drop on class-conditional ImageNet generation. We further show that our method can also be used as a drop-in technique to not only accelerate the popular pretrained stable diffusion (SD) models but also improve the prompt alignment of stylized SD models from the public model zoo. Code is released at https://github.com/NVlabs/T-Stitch},
	language = {en-US},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Pan, Zizheng and Zhuang, Bohan and Huang, De-An and Nie, Weili and Yu, Zhiding and Xiao, Chaowei and Cai, Jianfei and Anandkumar, Anima},
	month = feb,
	year = {2024},
	note = {arXiv:2402.14167 [cs]},
}

@misc{he_zipvl_2024,
	title = {{ZipVL}: {Efficient} {Large} {Vision}-{Language} {Models} with {Dynamic} {Token} {Sparsification}},
	shorttitle = {{ZipVL}},
	url = {http://arxiv.org/abs/2410.08584},
	doi = {10.48550/arXiv.2410.08584},
	abstract = {The efficiency of large vision-language models (LVLMs) is constrained by the computational bottleneck of the attention mechanism during the prefill phase and the memory bottleneck of fetching the key-value (KV) cache in the decoding phase, particularly in scenarios involving high-resolution images or videos. Visual content often exhibits substantial redundancy, resulting in highly sparse attention maps within LVLMs. This sparsity can be leveraged to accelerate attention computation or compress the KV cache through various approaches. However, most studies focus on addressing only one of these bottlenecks and do not adequately support dynamic adjustment of sparsity concerning distinct layers or tasks. In this paper, we present ZipVL, an efficient inference framework designed for LVLMs through a dynamic ratio allocation strategy of important tokens. This ratio is adaptively determined based on the layer-specific distribution of attention scores, rather than fixed hyper-parameters, thereby improving efficiency for less complex tasks while maintaining high performance for more challenging ones. Then we select important tokens based on their normalized attention scores and perform sparse attention mechanism solely on those important tokens, reducing the latency in the prefill phase. Tokens deemed less important will be discarded to reduce KV cache size, alleviating the memory bottleneck in the decoding phase. Our experiments demonstrate that ZipVL can accelerate the prefill phase by 2.3\${\textbackslash}times\$ and improve decoding throughput by 2.8\${\textbackslash}times\$, with a minimal accuracy reduction of only 0.5{\textbackslash}\% on VQAv2 benchmark over LLaVA-Next-13B model, effectively enhancing the generation efficiency of LVLMs.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {He, Yefei and Chen, Feng and Liu, Jing and Shao, Wenqi and Zhou, Hong and Zhang, Kaipeng and Zhuang, Bohan},
	month = dec,
	year = {2024},
	note = {arXiv:2410.08584 [cs]},
}

@inproceedings{yang2024objectaware,
title     = {Object-Aware Inversion and Reassembly for Image Editing},
author    = {Zhen Yang and Ganggui Ding and Wen Wang and Hao Chen and Bohan Zhuang and Chunhua Shen},
booktitle = {The Twelfth International Conference on Learning Representations},
year      = {2024},
url       = {https://openreview.net/forum?id=dpcVXiMlcv}
}

@article{chen2024mvsplat360,
    title     = {MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views},
    author    = {Chen, Yuedong and Zheng, Chuanxia and Xu, Haofei and Zhuang, Bohan and Vedaldi, Andrea and Cham, Tat-Jen and Cai, Jianfei},
    booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
    year      = {2024},
}

@misc{liu_focusformer_2022,
	title = {{FocusFormer}: {Focusing} on {What} {We} {Need} via {Architecture} {Sampler}},
	shorttitle = {{FocusFormer}},
	url = {http://arxiv.org/abs/2208.10861},
	doi = {10.48550/arXiv.2208.10861},
	abstract = {Vision Transformers (ViTs) have underpinned the recent breakthroughs in computer vision. However, designing the architectures of ViTs is laborious and heavily relies on expert knowledge. To automate the design process and incorporate deployment flexibility, one-shot neural architecture search decouples the supernet training and architecture specialization for diverse deployment scenarios. To cope with an enormous number of sub-networks in the supernet, existing methods treat all architectures equally important and randomly sample some of them in each update step during training. During architecture search, these methods focus on finding architectures on the Pareto frontier of performance and resource consumption, which forms a gap between training and deployment. In this paper, we devise a simple yet effective method, called FocusFormer, to bridge such a gap. To this end, we propose to learn an architecture sampler to assign higher sampling probabilities to those architectures on the Pareto frontier under different resource constraints during supernet training, making them sufficiently optimized and hence improving their performance. During specialization, we can directly use the well-trained architecture sampler to obtain accurate architectures satisfying the given resource constraint, which significantly improves the search efficiency. Extensive experiments on CIFAR-100 and ImageNet show that our FocusFormer is able to improve the performance of the searched architectures while significantly reducing the search cost. For example, on ImageNet, our FocusFormer-Ti with 1.4G FLOPs outperforms AutoFormer-Ti by 0.5\% in terms of the Top-1 accuracy.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Liu, Jing and Cai, Jianfei and Zhuang, Bohan},
	month = aug,
	year = {2022},
	note = {arXiv:2208.10861 [cs]},
}

@misc{wang_switchgpt_2023,
	title = {{SwitchGPT}: {Adapting} {Large} {Language} {Models} for {Non}-{Text} {Outputs}},
	shorttitle = {{SwitchGPT}},
	url = {http://arxiv.org/abs/2309.07623},
	doi = {10.48550/arXiv.2309.07623},
	abstract = {Large Language Models (LLMs), primarily trained on text-based datasets, exhibit exceptional proficiencies in understanding and executing complex linguistic instructions via text outputs. However, they falter when requests to generate non-text ones. Concurrently, modality conversion models, such as text-to-image, despite generating high-quality images, suffer from a lack of extensive textual pretraining. As a result, these models are only capable of accommodating specific image descriptions rather than comprehending more complex instructions. To bridge this gap, we propose a novel approach, {\textbackslash}methodname, from a modality conversion perspective that evolves a text-based LLM into a multi-modal one. We specifically employ a minimal dataset to instruct LLMs to recognize the intended output modality as directed by the instructions. Consequently, the adapted LLM can effectively summon various off-the-shelf modality conversion models from the model zoos to generate non-text responses. This circumvents the necessity for complicated pretraining that typically requires immense quantities of paired multi-modal data, while simultaneously inheriting the extensive knowledge of LLMs and the ability of high-quality generative models. To evaluate and compare the adapted multi-modal LLM with its traditional counterparts, we have constructed a multi-modal instruction benchmark that solicits diverse modality outputs. The experiment results reveal that, with minimal training, LLMs can be conveniently adapted to comprehend requests for non-text responses, thus achieving higher flexibility in multi-modal scenarios. Code and data will be made available at https://github.com/xinke-wang/SwitchGPT.},
	language = {en-US},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Wang, Xinyu and Zhuang, Bohan and Wu, Qi},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07623 [cs]},
}

@misc{liu_me-switch_2024,
	title = {{ME}-{Switch}: {A} {Memory}-{Efficient} {Expert} {Switching} {Framework} for {Large} {Language} {Models}},
	shorttitle = {{ME}-{Switch}},
	url = {http://arxiv.org/abs/2406.09041},
	doi = {10.48550/arXiv.2406.09041},
	abstract = {LLM development involves pre-training a foundation model on massive data, followed by fine-tuning on task-specific data to create specialized experts. Serving these experts can pose significant memory challenges, as loading all experts onto devices is impractical, and frequent switching between experts in response to user requests can incur substantial I/O costs. Previous approaches decompose the expert weights as the pre-trained weights plus delta weights, followed by quantizing the delta weights using output channel-wise step sizes to reduce the model size. However, these methods overlook the fact that certain input channels of delta weights can cause significant quantization errors at extremely low bitwidths. Additionally, existing methods assume that the appropriate model for a user request is known in advance, which is not the case in practice. To this end, we introduce ME-Switch, a memory-efficient expert switching framework tailored for serving multiple LLMs. To condense the number of bits required for describing the delta weights, we propose a salient-aware delta compression method that identifies salient input channels based on reconstruction error and applies mixed-precision quantization, reducing non-salient channels to low bits while keeping salient ones intact, cutting storage demand without compromising performance. Moreover, we develop a model-level routing method that efficiently directs user queries to the most suitable expert by performing domain classification. Extensive experiments show the promising memory efficiency and routing performance of ME-Switch. For example, when serving three models from the Mistral-7B family, ME-Switch reduces the model size by \$1.74{\textbackslash}times\$ and maintains nearly lossless performance on instruction, mathematical reasoning, and code generation tasks. Notably, our method can efficiently serve 16 Mistral-7B models on a single NVIDIA A100 GPU.},
	language = {en-US},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Liu, Jing and Gong, Ruihao and Zhang, Mingyang and He, Yefei and Cai, Jianfei and Zhuang, Bohan},
	month = oct,
	year = {2024},
	note = {arXiv:2406.09041 [cs]},
}

@misc{he_zipar_2024,
	title = {{ZipAR}: {Accelerating} {Auto}-regressive {Image} {Generation} through {Spatial} {Locality}},
	shorttitle = {{ZipAR}},
	url = {http://arxiv.org/abs/2412.04062},
	doi = {10.48550/arXiv.2412.04062},
	abstract = {In this paper, we propose ZipAR, a training-free, plug-and-play parallel decoding framework for accelerating auto-regressive (AR) visual generation. The motivation stems from the observation that images exhibit local structures, and spatially distant regions tend to have minimal interdependence. Given a partially decoded set of visual tokens, in addition to the original next-token prediction scheme in the row dimension, the tokens corresponding to spatially adjacent regions in the column dimension can be decoded in parallel, enabling the ``next-set prediction'' paradigm. By decoding multiple tokens simultaneously in a single forward pass, the number of forward passes required to generate an image is significantly reduced, resulting in a substantial improvement in generation efficiency. Experiments demonstrate that ZipAR can reduce the number of model forward passes by up to 91\% on the Emu3-Gen model without requiring any additional retraining. Code is available here: https://github.com/ThisisBillhe/ZipAR.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {He, Yefei and Chen, Feng and He, Yuanyu and He, Shaoxuan and Zhou, Hong and Zhang, Kaipeng and Zhuang, Bohan},
	month = dec,
	year = {2024},
	note = {arXiv:2412.04062 [cs]},
}

@misc{chen_enhancing_2024,
	title = {Enhancing {Perception} {Capabilities} of {Multimodal} {LLMs} with {Training}-{Free} {Fusion}},
	url = {http://arxiv.org/abs/2412.01289},
	doi = {10.48550/arXiv.2412.01289},
	abstract = {Multimodal LLMs (MLLMs) equip language models with visual capabilities by aligning vision encoders with language models. Existing methods to enhance the visual perception of MLLMs often involve designing more powerful vision encoders, which requires exploring a vast design space and re-aligning each potential encoder with the language model, resulting in prohibitively high training costs. In this paper, we introduce VisionFuse, a novel integration framework that efficiently utilizes multiple vision encoders from off-the-shelf MLLMs to enhance visual perception without requiring additional training. Our approach is motivated by the observation that different MLLMs tend to focus on distinct regions given the same query and image. Moreover, we find that the feature distributions of vision encoders within an MLLM family, a group of MLLMs sharing the same pretrained LLM, are highly aligned. Building on these insights, VisionFuse enriches the visual context by concatenating the tokens generated by the vision encoders of selected MLLMs within a family. By merging the parameters of language models from these MLLMs, VisionFuse allows a single language model to align with various vision encoders, significantly reducing deployment overhead. We conduct comprehensive evaluations across multiple multimodal benchmarks using various MLLM combinations, demonstrating substantial improvements in multimodal tasks. Notably, when integrating MiniGemini-8B and SLIME-8B, VisionFuse achieves an average performance increase of over 4\%.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Chen, Zhuokun and Hu, Jinwu and Deng, Zeshuai and Wang, Yufeng and Zhuang, Bohan and Tan, Mingkui},
	month = dec,
	year = {2024},
	note = {arXiv:2412.01289 [cs]},
}

@misc{chen_evaluating_2024,
	title = {Evaluating and {Advancing} {Multimodal} {Large} {Language} {Models} in {Ability} {Lens}},
	url = {http://arxiv.org/abs/2411.14725},
	doi = {10.48550/arXiv.2411.14725},
	abstract = {As multimodal large language models (MLLMs) advance rapidly, rigorous evaluation has become essential, providing further guidance for their development. In this work, we focus on a unified and robust evaluation of {\textbackslash}textbf\{vision perception\} abilities, the foundational skill of MLLMs. We find that existing perception benchmarks, each focusing on different question types, domains, and evaluation metrics, introduce significant evaluation variance, complicating comprehensive assessments of perception abilities when relying on any single benchmark. To address this, we introduce {\textbackslash}textbf\{AbilityLens\}, a unified benchmark designed to evaluate MLLMs across six key perception abilities, focusing on both accuracy and stability, with each ability encompassing diverse question types, domains, and metrics. With the assistance of AbilityLens, we: (1) identify the strengths and weaknesses of current models, highlighting stability patterns and revealing a notable performance gap between open-source and closed-source models; (2) introduce an online evaluation mode, which uncovers interesting ability conflict and early convergence phenomena during MLLM training; and (3) design a simple ability-specific model merging method that combines the best ability checkpoint from early training stages, effectively mitigating performance decline due to ability conflict. The benchmark and online leaderboard will be released soon.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Chen, Feng and Gou, Chenhui and Liu, Jing and Yang, Yang and Li, Zhaoyang and Zhang, Jiyuan and Sun, Zhenbang and Zhuang, Bohan and Wu, Qi},
	month = nov,
	year = {2024},
	note = {arXiv:2411.14725 [cs]},
}

@misc{wang_are_2025,
	title = {Are {Large} {Vision} {Language} {Models} {Good} {Game} {Players}?},
	url = {http://arxiv.org/abs/2503.02358},
	doi = {10.48550/arXiv.2503.02358},
	abstract = {Large Vision Language Models (LVLMs) have demonstrated remarkable abilities in understanding and reasoning about both visual and textual information. However, existing evaluation methods for LVLMs, primarily based on benchmarks like Visual Question Answering and image captioning, often fail to capture the full scope of LVLMs' capabilities. These benchmarks are limited by issues such as inadequate assessment of detailed visual perception, data contamination, and a lack of focus on multi-turn reasoning. To address these challenges, we propose {\textbackslash}method\{\}, a game-based evaluation framework designed to provide a comprehensive assessment of LVLMs' cognitive and reasoning skills in structured environments. {\textbackslash}method\{\} uses a set of games to evaluate LVLMs on four core tasks: Perceiving, Question Answering, Rule Following, and End-to-End Playing, with each target task designed to assess specific abilities, including visual perception, reasoning, decision-making, etc. Based on this framework, we conduct extensive experiments that explore the limitations of current LVLMs, such as handling long structured outputs and perceiving detailed and dense elements. Code and data are publicly available at https://github.com/xinke-wang/LVLM-Playground.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Wang, Xinyu and Zhuang, Bohan and Wu, Qi},
	month = mar,
	year = {2025},
	note = {arXiv:2503.02358 [cs]},
}

@misc{zhang_channel_2024,
	title = {Channel {Merging}: {Preserving} {Specialization} for {Merged} {Experts}},
	shorttitle = {Channel {Merging}},
	url = {http://arxiv.org/abs/2412.15283},
	doi = {10.48550/arXiv.2412.15283},
	abstract = {Lately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53\% parameters when used with a task-specific router.},
	urldate = {2025-03-14},
	publisher = {arXiv},
	author = {Zhang, Mingyang and Liu, Jing and Ding, Ganggui and Yu, Xinyi and Ou, Linlin and Zhuang, Bohan},
	month = dec,
	year = {2024},
	note = {arXiv:2412.15283 [cs]},
}

@article{pan_fast_2022,
	title = {Fast vision transformers with hilo attention},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/5d5f703ee1dedbfe324b1872f44db939-Abstract-Conference.html},
	language = {en-US},
	urldate = {2025-03-14},
	journal = {Advances in Neural Information Processing Systems},
	author = {Pan, Zizheng and Cai, Jianfei and Zhuang, Bohan},
	year = {2022},
	pages = {14541--14554},
}

@article{liu_discrimination-aware_2021,
	title = {Discrimination-aware network pruning for deep model compression},
	volume = {44},
	url = {https://ieeexplore.ieee.org/abstract/document/9384353/},
	language = {en-US},
	number = {8},
	urldate = {2025-03-14},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liu, Jing and Zhuang, Bohan and Zhuang, Zhuangwei and Guo, Yong and Huang, Junzhou and Zhu, Jinhui and Tan, Mingkui},
	year = {2021},
	note = {Publisher: IEEE},
	pages = {4035--4051},
}

@article{he_pruning_2024,
	title = {Pruning self-attentions into convolutional layers in single path},
	volume = {46},
	url = {https://ieeexplore.ieee.org/abstract/document/10409620/},
	number = {5},
	urldate = {2025-03-14},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {He, Haoyu and Cai, Jianfei and Liu, Jing and Pan, Zizheng and Zhang, Jing and Tao, Dacheng and Zhuang, Bohan},
	year = {2024},
	note = {Publisher: IEEE},
	pages = {3910--3922},
}

@article{liu_ecoformer_2022,
	title = {Ecoformer: {Energy}-saving attention with linear complexity},
	volume = {35},
	shorttitle = {Ecoformer},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/4310ae054ce265e56d8ea897971149b5-Abstract-Conference.html},
	urldate = {2025-03-14},
	journal = {Advances in Neural Information Processing Systems},
	author = {Liu, Jing and Pan, Zizheng and He, Haoyu and Cai, Jianfei and Zhuang, Bohan},
	year = {2022},
	pages = {10295--10308},
}

@article{liu_minicache_2024,
	title = {Minicache: {Kv} cache compression in depth dimension for large language models},
	volume = {37},
	shorttitle = {Minicache},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/fd0705710bf01b88a60a3d479ea341d9-Abstract-Conference.html},
	urldate = {2025-03-14},
	journal = {Advances in Neural Information Processing Systems},
	author = {Liu, Akide and Liu, Jing and Pan, Zizheng and He, Yefei and Haffari, Reza and Zhuang, Bohan},
	year = {2024},
	pages = {139997--140031},
}

@article{deng_efficient_2023,
	title = {Efficient test-time adaptation for super-resolution with second-order degradation and reconstruction},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/ec3d49763c653ad7c8d587f52220c129-Abstract-Conference.html},
	urldate = {2025-03-14},
	journal = {Advances in Neural Information Processing Systems},
	author = {Deng, Zeshuai and Chen, Zhuokun and Niu, Shuaicheng and Li, Thomas and Zhuang, Bohan and Tan, Mingkui},
	year = {2023},
	pages = {74671--74701},
}

@article{weng_mask_2023,
	title = {Mask propagation for efficient video semantic segmentation},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/167bcf2af2cd08fcf75b932022db0311-Abstract-Conference.html},
	language = {en-US},
	urldate = {2025-03-14},
	journal = {Advances in Neural Information Processing Systems},
	author = {Weng, Yuetian and Han, Mingfei and He, Haoyu and Li, Mingjie and Yao, Lina and Chang, Xiaojun and Zhuang, Bohan},
	year = {2023},
	pages = {7170--7183},
}

@article{liu_single-path_2023,
	title = {Single-path bit sharing for automatic loss-aware model compression},
	volume = {45},
	url = {https://ieeexplore.ieee.org/abstract/document/10122994/},
	number = {10},
	urldate = {2025-03-14},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liu, Jing and Zhuang, Bohan and Chen, Peng and Shen, Chunhua and Cai, Jianfei and Tan, Mingkui},
	year = {2023},
	note = {Publisher: IEEE},
	pages = {12459--12473},
}

@article{he_end--end_2023,
	title = {End-to-end one-shot human parsing},
	volume = {45},
	url = {https://ieeexplore.ieee.org/abstract/document/10207820/},
	number = {12},
	urldate = {2025-03-14},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {He, Haoyu and Zhang, Jing and Zhuang, Bohan and Cai, Jianfei and Tao, Dacheng},
	year = {2023},
	note = {Publisher: IEEE},
	pages = {14481--14496},
}

@article{chen_gmai-mmbench_2024,
	title = {{GMAI}-{MMBench}: {A} {Comprehensive} {Multimodal} {Evaluation} {Benchmark} {Towards} {General} {Medical} {AI}},
	volume = {37},
	shorttitle = {{GMAI}-{MMBench}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/ab7e02fd60e47e2a379d567f6b54f04e-Abstract-Datasets_and_Benchmarks_Track.html},
	language = {en},
	urldate = {2025-03-14},
	journal = {Advances in Neural Information Processing Systems},
	author = {Chen, Pengcheng and Ye, Jin and Wang, Guoan and Li, Yanjun and Deng, Zhongying and Li, Wei and Li, Tianbin and Duan, Haodong and Huang, Ziyan and Su, Yanzhou and Wang, Benyou and Zhang, Shaoting and Fu, Bin and Cai, Jianfei and Zhuang, Bohan and Seibel, Eric J. and Qiao, Yu and He, Junjun},
	month = dec,
	year = {2024},
	pages = {94327--94427},
}

@inproceedings{pan_scalable_2021,
	title = {Scalable vision transformers with hierarchical pooling},
	url = {http://openaccess.thecvf.com/content/ICCV2021/html/Pan_Scalable_Vision_Transformers_With_Hierarchical_Pooling_ICCV_2021_paper.html},
	language = {en-US},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/cvf international conference on computer vision},
	author = {Pan, Zizheng and Zhuang, Bohan and Liu, Jing and He, Haoyu and Cai, Jianfei},
	year = {2021},
	pages = {377--386},
}

@incollection{leonardis_mvsplat_2025,
	address = {Cham},
	title = {{MVSplat}: {Efficient} {3D} {Gaussian} {Splatting} from {Sparse} {Multi}-view {Images}},
	volume = {15079},
	isbn = {978-3-031-72663-7 978-3-031-72664-4},
	shorttitle = {{MVSplat}},
	url = {https://link.springer.com/10.1007/978-3-031-72664-4_21},
	language = {en},
	urldate = {2025-03-14},
	booktitle = {Computer {Vision} – {ECCV} 2024},
	publisher = {Springer Nature Switzerland},
	author = {Chen, Yuedong and Xu, Haofei and Zheng, Chuanxia and Zhuang, Bohan and Pollefeys, Marc and Geiger, Andreas and Cham, Tat-Jen and Cai, Jianfei},
	editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
	year = {2025},
	doi = {10.1007/978-3-031-72664-4_21},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {370--386},
}

@inproceedings{pan_less_2022,
	title = {Less is more: {Pay} less attention in vision transformers},
	volume = {36},
	shorttitle = {Less is more},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20099},
	language = {en-US},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {AAAI} conference on artificial intelligence},
	author = {Pan, Zizheng and Zhuang, Bohan and He, Haoyu and Liu, Jing and Cai, Jianfei},
	year = {2022},
	note = {Issue: 2},
	pages = {2035--2043},
}

@inproceedings{zhang_motion_2024,
	title = {Motion mamba: {Efficient} and long sequence motion generation},
	shorttitle = {Motion mamba},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Zhang, Zeyu and Liu, Akide and Reid, Ian and Hartley, Richard and Zhuang, Bohan and Tang, Hao},
	year = {2024},
	pages = {265--282},
}

@inproceedings{he_sensitivity-aware_2023,
	title = {Sensitivity-aware visual parameter-efficient fine-tuning},
	url = {http://openaccess.thecvf.com/content/ICCV2023/html/He_Sensitivity-Aware_Visual_Parameter-Efficient_Fine-Tuning_ICCV_2023_paper.html},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {He, Haoyu and Cai, Jianfei and Zhang, Jing and Tao, Dacheng and Zhuang, Bohan},
	year = {2023},
	pages = {11825--11835},
}

@inproceedings{li_automated_2022,
	title = {Automated progressive learning for efficient training of vision transformers},
	url = {http://openaccess.thecvf.com/content/CVPR2022/html/Li_Automated_Progressive_Learning_for_Efficient_Training_of_Vision_Transformers_CVPR_2022_paper.html},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Li, Changlin and Zhuang, Bohan and Wang, Guangrun and Liang, Xiaodan and Chang, Xiaojun and Yang, Yi},
	year = {2022},
	pages = {12486--12496},
}

@inproceedings{he_bivit_2023,
	title = {Bivit: {Extremely} compressed binary vision transformers},
	shorttitle = {Bivit},
	url = {http://openaccess.thecvf.com/content/ICCV2023/html/He_BiViT_Extremely_Compressed_Binary_Vision_Transformers_ICCV_2023_paper.html},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {He, Yefei and Lou, Zhenyu and Zhang, Luoming and Liu, Jing and Wu, Weijia and Zhou, Hong and Zhuang, Bohan},
	year = {2023},
	pages = {5651--5663},
}

@inproceedings{chen_aqd_2021,
	title = {Aqd: {Towards} accurate quantized object detection},
	shorttitle = {Aqd},
	url = {http://openaccess.thecvf.com/content/CVPR2021/html/Chen_AQD_Towards_Accurate_Quantized_Object_Detection_CVPR_2021_paper.html},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Chen, Peng and Liu, Jing and Zhuang, Bohan and Tan, Mingkui and Shen, Chunhua},
	year = {2021},
	pages = {104--113},
}

@incollection{avidan_efficient_2022,
	address = {Cham},
	title = {An {Efficient} {Spatio}-{Temporal} {Pyramid} {Transformer} for {Action} {Detection}},
	volume = {13694},
	isbn = {978-3-031-19829-8 978-3-031-19830-4},
	url = {https://link.springer.com/10.1007/978-3-031-19830-4_21},
	language = {en},
	urldate = {2025-03-14},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Weng, Yuetian and Pan, Zizheng and Han, Mingfei and Chang, Xiaojun and Zhuang, Bohan},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-19830-4_21},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {358--375},
}

@incollection{leonardis_longvlm_2025,
	address = {Cham},
	title = {{LongVLM}: {Efficient} {Long} {Video} {Understanding} via {Large} {Language} {Models}},
	volume = {15091},
	isbn = {978-3-031-73413-7 978-3-031-73414-4},
	shorttitle = {{LongVLM}},
	url = {https://link.springer.com/10.1007/978-3-031-73414-4_26},
	language = {en},
	urldate = {2025-03-14},
	booktitle = {Computer {Vision} – {ECCV} 2024},
	publisher = {Springer Nature Switzerland},
	author = {Weng, Yuetian and Han, Mingfei and He, Haoyu and Chang, Xiaojun and Zhuang, Bohan},
	editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
	year = {2025},
	doi = {10.1007/978-3-031-73414-4_26},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {453--470},
}

@inproceedings{pan_stitchable_2023,
	title = {Stitchable neural networks},
	url = {http://openaccess.thecvf.com/content/CVPR2023/html/Pan_Stitchable_Neural_Networks_CVPR_2023_paper.html},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Pan, Zizheng and Cai, Jianfei and Zhuang, Bohan},
	year = {2023},
	pages = {16102--16112},
}

@inproceedings{liu_sa-bnn_2021,
	title = {{SA}-{BNN}: {State}-aware binary neural network},
	volume = {35},
	shorttitle = {{SA}-{BNN}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16306},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Liu, Chunlei and Chen, Peng and Zhuang, Bohan and Shen, Chunhua and Zhang, Baochang and Ding, Wenrui},
	year = {2021},
	note = {Issue: 3},
	pages = {2091--2099},
}

@inproceedings{wang_modaverse_2024,
	title = {Modaverse: {Efficiently} transforming modalities with llms},
	shorttitle = {Modaverse},
	url = {http://openaccess.thecvf.com/content/CVPR2024/html/Wang_ModaVerse_Efficiently_Transforming_Modalities_with_LLMs_CVPR_2024_paper.html},
	language = {en-US},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Wang, Xinyu and Zhuang, Bohan and Wu, Qi},
	year = {2024},
	pages = {26606--26616},
}

@inproceedings{he_dynamic_2023,
	title = {Dynamic focus-aware positional queries for semantic segmentation},
	url = {http://openaccess.thecvf.com/content/CVPR2023/html/He_Dynamic_Focus-Aware_Positional_Queries_for_Semantic_Segmentation_CVPR_2023_paper.html},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {He, Haoyu and Cai, Jianfei and Pan, Zizheng and Liu, Jing and Zhang, Jing and Tao, Dacheng and Zhuang, Bohan},
	year = {2023},
	pages = {11299--11308},
}

@inproceedings{chen_fatnn_2021,
	title = {{FATNN}: {Fast} and accurate ternary neural networks},
	shorttitle = {{FATNN}},
	url = {http://openaccess.thecvf.com/content/ICCV2021/html/Chen_FATNN_Fast_and_Accurate_Ternary_Neural_Networks_ICCV_2021_paper.html},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Chen, Peng and Zhuang, Bohan and Shen, Chunhua},
	year = {2021},
	pages = {5219--5228},
}

@incollection{linguraru_sam-med3d-moe_2024,
	address = {Cham},
	title = {{SAM}-{Med3D}-{MoE}: {Towards} a {Non}-{Forgetting} {Segment} {Anything} {Model} via {Mixture} of {Experts} for {3D} {Medical} {Image} {Segmentation}},
	volume = {15009},
	isbn = {978-3-031-72113-7 978-3-031-72114-4},
	shorttitle = {{SAM}-{Med3D}-{MoE}},
	url = {https://link.springer.com/10.1007/978-3-031-72114-4_53},
	language = {en},
	urldate = {2025-03-14},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2024},
	publisher = {Springer Nature Switzerland},
	author = {Wang, Guoan and Ye, Jin and Cheng, Junlong and Li, Tianbin and Chen, Zhaolin and Cai, Jianfei and He, Junjun and Zhuang, Bohan},
	editor = {Linguraru, Marius George and Dou, Qi and Feragen, Aasa and Giannarou, Stamatia and Glocker, Ben and Lekadir, Karim and Schnabel, Julia A.},
	year = {2024},
	doi = {10.1007/978-3-031-72114-4_53},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {552--561},
}

@inproceedings{he_efficient_2024,
	title = {Efficient stitchable task adaptation},
	url = {http://openaccess.thecvf.com/content/CVPR2024/html/He_Efficient_Stitchable_Task_Adaptation_CVPR_2024_paper.html},
	urldate = {2025-03-14},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {He, Haoyu and Pan, Zizheng and Liu, Jing and Cai, Jianfei and Zhuang, Bohan},
	year = {2024},
	pages = {28555--28565},
}

@incollection{leonardis_stitched_2025,
	address = {Cham},
	title = {Stitched {ViTs} are {Flexible} {Vision} {Backbones}},
	volume = {15099},
	isbn = {978-3-031-72939-3 978-3-031-72940-9},
	url = {https://link.springer.com/10.1007/978-3-031-72940-9_15},
	language = {en},
	urldate = {2025-03-14},
	booktitle = {Computer {Vision} – {ECCV} 2024},
	publisher = {Springer Nature Switzerland},
	author = {Pan, Zizheng and Liu, Jing and He, Haoyu and Cai, Jianfei and Zhuang, Bohan},
	editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
	year = {2025},
	doi = {10.1007/978-3-031-72940-9_15},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {258--274},
}
@article{zhuang2021effective,
  title={Effective training of convolutional neural networks with low-bitwidth weights and activations},
  author={Zhuang, Bohan and Tan, Mingkui and Liu, Jing and Liu, Lingqiao and Reid, Ian and Shen, Chunhua},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={10},
  pages={6140--6152},
  year={2021},
  publisher={IEEE}
}
@inproceedings{zhuang2019structured,
  title={Structured binary neural networks for accurate image classification and semantic segmentation},
  author={Zhuang, Bohan and Shen, Chunhua and Tan, Mingkui and Liu, Lingqiao and Reid, Ian},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={413--422},
  year={2019}
}

