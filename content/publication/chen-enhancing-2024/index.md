---
title: Enhancing Perception Capabilities of Multimodal LLMs with Training-Free Fusion
authors:
- Zhuokun Chen
- Jinwu Hu
- Zeshuai Deng
- Yufeng Wang
- Bohan Zhuang
- Mingkui Tan
date: '2024-12-01'
publishDate: '2025-03-14T12:48:48.083171Z'
publication_types:
- manuscript
publication: '*arXiv*'
doi: 10.48550/arXiv.2412.01289
abstract: Multimodal LLMs (MLLMs) equip language models with visual capabilities by
  aligning vision encoders with language models. Existing methods to enhance the visual
  perception of MLLMs often involve designing more powerful vision encoders, which
  requires exploring a vast design space and re-aligning each potential encoder with
  the language model, resulting in prohibitively high training costs. In this paper,
  we introduce VisionFuse, a novel integration framework that efficiently utilizes
  multiple vision encoders from off-the-shelf MLLMs to enhance visual perception without
  requiring additional training. Our approach is motivated by the observation that
  different MLLMs tend to focus on distinct regions given the same query and image.
  Moreover, we find that the feature distributions of vision encoders within an MLLM
  family, a group of MLLMs sharing the same pretrained LLM, are highly aligned. Building
  on these insights, VisionFuse enriches the visual context by concatenating the tokens
  generated by the vision encoders of selected MLLMs within a family. By merging the
  parameters of language models from these MLLMs, VisionFuse allows a single language
  model to align with various vision encoders, significantly reducing deployment overhead.
  We conduct comprehensive evaluations across multiple multimodal benchmarks using
  various MLLM combinations, demonstrating substantial improvements in multimodal
  tasks. Notably, when integrating MiniGemini-8B and SLIME-8B, VisionFuse achieves
  an average performance increase of over 4%.
links:
- name: URL
  url: http://arxiv.org/abs/2412.01289
---
