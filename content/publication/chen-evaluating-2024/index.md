---
title: Evaluating and Advancing Multimodal Large Language Models in Ability Lens
authors:
- Feng Chen
- Chenhui Gou
- Jing Liu
- Yang Yang
- Zhaoyang Li
- Jiyuan Zhang
- Zhenbang Sun
- Bohan Zhuang
- Qi Wu
date: '2024-11-01'
publishDate: '2025-03-14T12:48:48.086076Z'
publication_types:
- manuscript
publication: '*arXiv*'
doi: 10.48550/arXiv.2411.14725
abstract: 'As multimodal large language models (MLLMs) advance rapidly, rigorous evaluation
  has become essential, providing further guidance for their development. In this
  work, we focus on a unified and robust evaluation of textbfvision perception abilities,
  the foundational skill of MLLMs. We find that existing perception benchmarks, each
  focusing on different question types, domains, and evaluation metrics, introduce
  significant evaluation variance, complicating comprehensive assessments of perception
  abilities when relying on any single benchmark. To address this, we introduce textbfAbilityLens,
  a unified benchmark designed to evaluate MLLMs across six key perception abilities,
  focusing on both accuracy and stability, with each ability encompassing diverse
  question types, domains, and metrics. With the assistance of AbilityLens, we: (1)
  identify the strengths and weaknesses of current models, highlighting stability
  patterns and revealing a notable performance gap between open-source and closed-source
  models; (2) introduce an online evaluation mode, which uncovers interesting ability
  conflict and early convergence phenomena during MLLM training; and (3) design a
  simple ability-specific model merging method that combines the best ability checkpoint
  from early training stages, effectively mitigating performance decline due to ability
  conflict. The benchmark and online leaderboard will be released soon.'
links:
- name: URL
  url: http://arxiv.org/abs/2411.14725
---
